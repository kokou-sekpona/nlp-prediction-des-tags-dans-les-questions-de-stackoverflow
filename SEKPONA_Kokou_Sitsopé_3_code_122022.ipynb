{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aod15HntQ1Vj"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "english=words.words()\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "\n",
        "def process(doc):\n",
        "  #Tokenisation\n",
        "  tok=RegexpTokenizer(r'\\w+')\n",
        "  doc=tok.tokenize(doc)\n",
        "\n",
        "  #Supression des stopwords\n",
        "  doc=[i for i in doc if i not in stop_words]\n",
        "\n",
        "  #Supression des mots à moins de 3 caractères\n",
        "  doc=[i for i in doc if len(i)>=3]\n",
        "\n",
        "  #Lemmatisation\n",
        "  txt=WordNetLemmatizer()\n",
        "  doc=[txt.lemmatize(i) for i in doc]\n",
        "\n",
        "  #Ne garder que les mots anglais\n",
        "  doc=[i for i in doc if i in english]\n",
        "\n",
        "  #Ne concerver que les caractères alphanumériques\n",
        "  doc=[i for i in doc if i.isalpha() ]\n",
        "\n",
        "  #Supression des mots rares\n",
        "  doc=[i for i in doc if i not in rare1]\n",
        "\n",
        "  #Supression des mots qui se repetent de trop\n",
        "\n",
        "  #Supression des doublons\n",
        "  doc=pd.Series(doc).value_counts()\n",
        "  doc=list(doc.index)\n",
        "  #ou bien on peu faire\n",
        "  #doc=[i for i in set(doc)]\n",
        "  \n",
        "  try:\n",
        "\n",
        "     doc1=' '.join(doc)\n",
        "  except:\n",
        "     doc1=' '.join(['Not' ,'text'])\n",
        "\n",
        "  return doc1\n",
        "\n",
        "\n",
        "def out(sentence):\n",
        "    sentence=process(sentence)\n",
        "    predict=model.predict([sentence])\n",
        "    output=[]\n",
        "    for i, j in zip(predict[0], clas):\n",
        "      if i==1:\n",
        "        output.append(j)\n",
        "      else:\n",
        "        pass\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"classes.txt\", \"r\") as h:\n",
        "  clas=h.read()\n",
        "clas=clas.split(',')\n",
        "model=pickle.load(open('stack_model.pkl', 'rb'))\n",
        "\n",
        "with open(\"rare1.txt\", \"r\") as h:\n",
        "  rare1=h.read()\n",
        "rare1=rare1.split(',')"
      ],
      "metadata": {
        "id": "HZxE1VfFRLnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/tags\")\n",
        "async def predict(doc: str):\n",
        "    pred=out(doc)\n",
        "    return {\"Tags\": pred}"
      ],
      "metadata": {
        "id": "TsJBAhlHREin"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}